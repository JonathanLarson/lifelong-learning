{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble.forest import _generate_unsampled_indices\n",
    "from sklearn.ensemble.forest import _generate_sample_indices\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sympy.solvers import solve\n",
    "from sympy import Symbol\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from itertools import permutations\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "np.warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, 1]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file contains code for conditional entropy forests. \n",
    "Adapted from https://github.com/rguo123/conditional_entropy_forests/blob/master/code/algorithm.py\n",
    "\"\"\"\n",
    "\n",
    "def finite_sample_correction(class_probs, row_sums):\n",
    "    \n",
    "    where_0 = np.argwhere(class_probs == 0)\n",
    "    for elem in where_0:\n",
    "        class_probs[elem[0], elem[1]] = 1 / (2 * row_sums[elem[0], None])\n",
    "    where_1 = np.argwhere(class_probs == 1)\n",
    "    for elem in where_1:\n",
    "        class_probs[elem[0], elem[1]] = 1 - 1 / (2 * row_sums[elem[0], None])\n",
    "    \n",
    "    return class_probs\n",
    "\n",
    "def build_model(X, y, n_estimators=200, max_samples=.32,\n",
    "                                            bootstrap=True,\n",
    "                                            depth=30,\n",
    "                                            min_samples_leaf=1):\n",
    "    if X.ndim == 1:\n",
    "        raise ValueError('1d data will cause headaches down the road')\n",
    "        \n",
    "    max_features = int(np.ceil(np.sqrt(X.shape[1])))\n",
    "        \n",
    "    model=BaggingClassifier(DecisionTreeClassifier(max_depth=depth, min_samples_leaf=min_samples_leaf,\n",
    "                                                     max_features = max_features),\n",
    "                              n_estimators=n_estimators,\n",
    "                              max_samples=max_samples,\n",
    "                              bootstrap=bootstrap)\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "def get_leaves(estimator):\n",
    "    # adapted from https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html\n",
    "    n_nodes = estimator.tree_.node_count\n",
    "    children_left = estimator.tree_.children_left\n",
    "    children_right = estimator.tree_.children_right\n",
    "    feature = estimator.tree_.feature\n",
    "    threshold = estimator.tree_.threshold\n",
    "    \n",
    "    leaf_ids = []\n",
    "    stack = [(0, -1)] \n",
    "    while len(stack) > 0:\n",
    "        node_id, parent_depth = stack.pop()\n",
    "\n",
    "        # If we have a test node\n",
    "        if (children_left[node_id] != children_right[node_id]):\n",
    "            stack.append((children_left[node_id], parent_depth + 1))\n",
    "            stack.append((children_right[node_id], parent_depth + 1))\n",
    "        else:\n",
    "            leaf_ids.append(node_id)\n",
    "            \n",
    "    return np.array(leaf_ids)\n",
    "\n",
    "def estimate_posteriors(model, train, y, test, in_task=True, subsample=0, acorn=None):\n",
    "    if acorn is None:\n",
    "        acorn = np.random.randint(10**6)\n",
    "    np.random.seed(acorn)\n",
    "    \n",
    "    n, d = train.shape\n",
    "    m, d_ = test.shape\n",
    "    \n",
    "    if d != d_:\n",
    "        raise ValueError(\"train and test data in different dimensions\")\n",
    "    \n",
    "    class_counts = np.zeros((m, model.n_classes_))\n",
    "    for tree in model:\n",
    "        # get out of bag indicies\n",
    "        if in_task:\n",
    "            prob_indices = _generate_unsampled_indices(tree.random_state, n)\n",
    "            # in_bag_idx = _generate_sample_indices(tree.random_state, n) # this is not behaving as i expected\n",
    "        else:\n",
    "            if subsample:\n",
    "                prob_indices = np.random.choice(range(n), size=int(subsample*n), replace=False)\n",
    "            else:\n",
    "                prob_indices = range(n)\n",
    "        \n",
    "        leaf_nodes = get_leaves(tree)\n",
    "        unique_leaf_nodes = np.unique(leaf_nodes)\n",
    "            \n",
    "        # get all node counts\n",
    "        node_counts = tree.tree_.n_node_samples\n",
    "        # get probs for eval samples\n",
    "        posterior_class_counts = np.zeros((len(unique_leaf_nodes), model.n_classes_))\n",
    "\n",
    "        for prob_index in prob_indices:\n",
    "            temp_node = tree.apply(train[prob_index].reshape(1, -1)).item()\n",
    "            posterior_class_counts[np.where(unique_leaf_nodes == temp_node)[0][0], y[prob_index]] += 1\n",
    "            \n",
    "        # total number of points in a node\n",
    "        row_sums = posterior_class_counts.sum(axis=1)\n",
    "        \n",
    "        # no divide by zero\n",
    "        row_sums[row_sums == 0] = 1\n",
    "\n",
    "        # posteriors\n",
    "        class_probs = (posterior_class_counts / row_sums[:, None])\n",
    "        # posteriors with finite sampling correction\n",
    "        \n",
    "        class_probs = finite_sample_correction(class_probs, row_sums)\n",
    "    \n",
    "        # posteriors as a list\n",
    "        class_probs.tolist()\n",
    "        \n",
    "        partition_counts = np.asarray([node_counts[np.where(unique_leaf_nodes == x)[0][0]] for x in tree.apply(test)])\n",
    "        # get probability for out of bag samples\n",
    "        eval_class_probs = [class_probs[np.where(unique_leaf_nodes == x)[0][0]] for x in tree.apply(test)]\n",
    "        eval_class_probs = np.array(eval_class_probs)\n",
    "        # find total elements for out of bag samples\n",
    "        elems = np.multiply(eval_class_probs, partition_counts[:, np.newaxis])\n",
    "        # store counts for each x (repeat fhis for each tree)\n",
    "        class_counts += elems\n",
    "    # calculate p(y|X = x) for all x's\n",
    "    probs = class_counts / class_counts.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    return probs\n",
    "\n",
    "def predict(a):\n",
    "    return np.argmax(a, axis = 1).astype(int)\n",
    "\n",
    "\n",
    "def generate_sample(n, pi, conditional_0, params0, conditional_1, params1, d = 2, acorn=None):\n",
    "    if acorn is None:\n",
    "        acorn = np.random.seed(10**6)\n",
    "    np.random.seed(acorn)\n",
    "    \n",
    "    n0 = int(np.random.binomial(n, pi))\n",
    "    n1 = n - n0\n",
    "    ns = [n0, n1]\n",
    "    \n",
    "    X0 = conditional_0(*params0, size=(n0, d))\n",
    "    \n",
    "    X1 = conditional_1(*params1, size=(n1, d))\n",
    "    \n",
    "    labels = np.concatenate([i*np.ones(ns[i]) for i in range(len(ns))]).astype(int)\n",
    "    \n",
    "    return [np.concatenate((X0, X1), axis = 0), labels.astype(int)]\n",
    "\n",
    "def generate_parity(n, d=2, invert_labels=False,acorn=None):\n",
    "    if acorn is not None:\n",
    "        np.random.seed(acorn)\n",
    "        \n",
    "    X = np.random.uniform(-1, 1, size=(n, d))\n",
    "    Y = (np.sum(X > 0, axis=1) % 2 == 0).astype(int)\n",
    "    \n",
    "    return X, Y.astype(int)\n",
    "\n",
    "def generate_box(n, d=2, invert_labels=False, acorn=None):\n",
    "    x = Symbol('x')\n",
    "    sol = float(max(solve(x**d - 2*(1-x) + (1 - x)**d, x)))\n",
    "    \n",
    "    if acorn is not None:\n",
    "        np.random.seed(acorn)\n",
    "\n",
    "    X = np.random.uniform(-1, 1, size=(n, d))\n",
    "    \n",
    "    Y = -1*np.ones(n)\n",
    "    \n",
    "    for i in range(n):\n",
    "        for dim in range(d):\n",
    "            if X[i, dim] > sol or X[i, dim] < -sol:\n",
    "                Y[i] = 1\n",
    "        if Y[i] == -1:\n",
    "            Y[i] = 0\n",
    "\n",
    "    return X, Y.astype(int)\n",
    "\n",
    "def jovo_experiment(nx, nz, d, dist_x, dist_z, m, target=\"Z\", subsample = 0.32, metric='accuracy',n_algos=6):\n",
    "    metric_ = getattr(getattr(__import__('sklearn'), 'metrics'), '%s_score'%(metric))\n",
    "    \n",
    "    if target == \"Z\":\n",
    "        invert_z = True\n",
    "        invert_x = False\n",
    "    else:\n",
    "        invert_z = False\n",
    "        invert_x = True\n",
    "        \n",
    "    Tx = int(np.floor(np.sqrt(nx)))\n",
    "    Tz = int(np.floor(np.sqrt(nz)))\n",
    "    \n",
    "    Kx = int(np.floor(np.log(nx)))\n",
    "    Kz = int(np.floor(np.log(nz)))\n",
    "    \n",
    "    errors = np.zeros(n_algos)\n",
    "\n",
    "    # Source task\n",
    "    X, labelsX = dist_x(nx, d, invert_labels=invert_x)\n",
    "    testX, test_labelsX = dist_x(m, d, invert_labels=invert_x)\n",
    "\n",
    "    \n",
    "    # Target task\n",
    "    Z, labelsZ = dist_z(nz, d, invert_labels=invert_z)\n",
    "    testZ, test_labelsZ = dist_z(m, d, invert_labels=invert_z)\n",
    "\n",
    "\n",
    "    model_X = build_model(X, labelsX, Tx)\n",
    "    model_Z = build_model(Z, labelsZ, Tz)\n",
    "\n",
    "    posteriors_structX_estX=estimate_posteriors(model_X, X, labelsX, testX, in_task=True)\n",
    "    posteriors_structZ_estX=estimate_posteriors(model_Z, X, labelsX, testX, in_task=False, subsample=subsample)\n",
    "\n",
    "    pred_structX_estX=predict(posteriors_structX_estX)\n",
    "    pred_structZ_estX=predict(posteriors_structZ_estX)\n",
    "\n",
    "    posteriors_structX_estZ=estimate_posteriors(model_X, Z, labelsZ, testZ, in_task=False, subsample=subsample)\n",
    "    posteriors_structZ_estZ=estimate_posteriors(model_Z, Z, labelsZ, testZ, in_task=True)\n",
    "    \n",
    "    pred_structX_estZ=predict(posteriors_structX_estZ)\n",
    "    pred_structZ_estZ=predict(posteriors_structZ_estZ)\n",
    "\n",
    "    # calculate errors without attempting to transfer knowledge\n",
    "    pred_X = predict(posteriors_structX_estX)\n",
    "    pred_Z = predict(posteriors_structZ_estZ)\n",
    "\n",
    "    errors[0] = metric_(test_labelsX, pred_X) #1 - np.sum(test_labelsX == pred_X)/m\n",
    "    errors[3] = metric_(test_labelsZ, pred_Z) # (1 - np.sum(test_labelsZ == pred_Z)/m\n",
    "    \n",
    "#     errors[1] = 1 - np.sum(test_labelsX == pred_structZ_estX)/m\n",
    "#     errors[4] = 1 - np.sum(test_labelsZ == pred_structX_estZ)/m\n",
    "\n",
    "    # jtv ?\n",
    "    pred_X_jtv = predict(posteriors_structX_estX + posteriors_structZ_estX)\n",
    "    pred_Z_jtv = predict(posteriors_structZ_estZ + posteriors_structX_estZ)\n",
    "\n",
    "    errors[1] = metric_(test_labelsX, pred_X_jtv) # 1 - np.sum(test_labelsX == pred_X_jtv)/m\n",
    "    errors[4] = metric_(test_labelsZ, pred_Z_jtv) # 1 - np.sum(test_labelsZ == pred_Z_jtv)/m\n",
    "    \n",
    "    # Sum\n",
    "    X, labelsX = dist_x(nz + nx, d, invert_labels=invert_x)\n",
    "    \n",
    "    model_best_X = build_model(X, labelsX, int(np.floor(np.sqrt(nx + nz))))\n",
    "    \n",
    "    posteriors_best_X=estimate_posteriors(model_best_X, X, labelsX, testX, in_task=True)\n",
    "    predictions_best_X=predict(posteriors_best_X)\n",
    "    \n",
    "    errors[2] = metric_(test_labelsX, predictions_best_X) # 1 - np.sum(test_labelsX == predictions_best_X)/m\n",
    "    \n",
    "    # Sum\n",
    "    Z, labelsZ = dist_z(nz + nx, d, invert_labels=invert_z)\n",
    "    \n",
    "    model_best_Z = build_model(Z, labelsZ, int(np.floor(np.sqrt(nx + nz))))\n",
    "    \n",
    "    posteriors_best_Z=estimate_posteriors(model_best_Z, Z, labelsZ, testZ, in_task=True)\n",
    "    predictions_best_Z=predict(posteriors_best_Z)\n",
    "    \n",
    "    errors[5] = metric_(test_labelsZ, predictions_best_Z) #1 - np.sum(test_labelsZ == predictions_best_Z)/m\n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_box(100, 2)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 1/11 [00:04<00:45,  4.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 18%|█▊        | 2/11 [00:06<00:34,  3.81s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 27%|██▋       | 3/11 [00:09<00:27,  3.40s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 36%|███▋      | 4/11 [00:11<00:22,  3.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▌     | 5/11 [00:14<00:18,  3.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████▍    | 6/11 [00:18<00:16,  3.20s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 64%|██████▎   | 7/11 [00:21<00:13,  3.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████▎  | 8/11 [00:26<00:10,  3.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|████████▏ | 9/11 [00:30<00:07,  3.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 91%|█████████ | 10/11 [00:36<00:04,  4.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 11/11 [00:41<00:00,  4.78s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "np.random.seed(1)\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "mc_its = 100 # number of simulation repititions\n",
    "# ns0 = (10*np.arange(3,5,step=1)).astype(int)\n",
    "ns = (100*np.arange(0.5, 6, step=0.5)).astype(int) # number of training samples \n",
    "nz_prop = 1\n",
    "\n",
    "m = 100 # number of test samples each monte carlo iteration\n",
    "\n",
    "algorithms = ['Decision Forests Source', 'Lifelong Forests Source', 'Optimal Forests Source']\n",
    "algorithms = np.concatenate((algorithms, ['Decision Forests Target', 'Lifelong Forests Target', 'Optimal Forests Target']))\n",
    "\n",
    "M = len(algorithms)\n",
    "\n",
    "mean_precision= np.zeros((M, len(ns)))\n",
    "std_precision= np.zeros((M, len(ns)))\n",
    "\n",
    "d = 3\n",
    "metric = 'precision'\n",
    "np.random.seed(1)\n",
    "for j, n in enumerate(tqdm(ns)):\n",
    "    condensed_func = lambda x : jovo_experiment(200, x, d, generate_parity, generate_parity, m, metric=metric,\n",
    "                                                n_algos=M)\n",
    "    \n",
    "    precisions = np.array(Parallel(n_jobs=-2)(delayed(condensed_func)(int(x)) for x in n*np.ones(mc_its)))\n",
    "    \n",
    "    mean_precision[:, j] = np.mean(precisions, axis=0)\n",
    "    std_precision[:, j] = np.std(precisions, ddof=1, axis=0)\n",
    "\n",
    "mean_recall= np.zeros((M, len(ns)))\n",
    "std_recall= np.zeros((M, len(ns)))\n",
    "\n",
    "d = 3\n",
    "metric = 'recall'\n",
    "np.random.seed(1)\n",
    "for j, n in enumerate(tqdm(ns)):\n",
    "    condensed_func = lambda x : jovo_experiment(200, x, d, generate_parity, generate_parity, m, metric=metric,\n",
    "                                                n_algos=M)\n",
    "    \n",
    "    recalls = np.array(Parallel(n_jobs=-2)(delayed(condensed_func)(int(x)) for x in n*np.ones(mc_its)))\n",
    "    \n",
    "    mean_recall[:, j] = np.mean(recalls, axis=0)\n",
    "    std_recall[:, j] = np.std(recalls, ddof=1, axis=0)\n",
    "\n",
    "mean_accuracy= np.zeros((M, len(ns)))\n",
    "std_accuracy= np.zeros((M, len(ns)))\n",
    "\n",
    "d = 3\n",
    "metric = 'accuracy'\n",
    "np.random.seed(1)\n",
    "for j, n in enumerate(tqdm(ns)):\n",
    "    condensed_func = lambda x : jovo_experiment(200, x, d, generate_parity, generate_parity, m, metric=metric,\n",
    "                                                n_algos=M)\n",
    "    \n",
    "    accuracys = np.array(Parallel(n_jobs=-2)(delayed(condensed_func)(int(x)) for x in n*np.ones(mc_its)))\n",
    "    \n",
    "    mean_accuracy[:, j] = np.mean(accuracys, axis=0)\n",
    "    std_accuracy[:, j] = np.std(accuracys, ddof=1, axis=0)\n",
    "\n",
    "np.random.seed(2)\n",
    "X, labelsX = generate_parity(500, 2)\n",
    "temp_labelsX = []\n",
    "\n",
    "# Target task\n",
    "Z, labelsZ = generate_parity(500, 2, invert_labels=True)\n",
    "temp_labelsZ = []\n",
    "\n",
    "for i in range(len(labelsX)):\n",
    "    if labelsX[i] == 0:\n",
    "        temp_labelsX.append('k')\n",
    "    else:\n",
    "        temp_labelsX.append('pink')\n",
    "        \n",
    "    if labelsZ[i] == 0:\n",
    "        temp_labelsZ.append('pink')\n",
    "    else:\n",
    "        temp_labelsZ.append('k')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "mc_it = mc_its\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "colors = sns.color_palette(\"Set1\", n_colors = M)\n",
    "fig1, ax = plt.subplots(4,2, figsize = (13, 13), sharex = 'row', sharey='row')\n",
    "\n",
    "ax[0,0].scatter(X[:, 0], X[:, 1], c = temp_labelsX)\n",
    "ax[0,1].scatter(Z[:, 0], Z[:, 1], c = temp_labelsZ)\n",
    "\n",
    "\n",
    "ax[0,0].set_title('Source Task: Parity')\n",
    "ax[0,1].set_title('Target Task: Not Parity')\n",
    "    \n",
    "algo_offset=0\n",
    "for i, algo in enumerate(algorithms[algo_offset:-3]):\n",
    "    ax[1,0].plot(ns, mean_precision[i + algo_offset], label=algo, c=colors[i])\n",
    "    ax[1,0].fill_between(ns, \n",
    "            mean_precision[i + algo_offset] + 1.96*std_precision[i + algo_offset]/np.sqrt(mc_it), \n",
    "            mean_precision[i + algo_offset] - 1.96*std_precision[i + algo_offset]/np.sqrt(mc_it), \n",
    "            where=mean_precision[i + algo_offset] + 1.96*std_precision[i + algo_offset]/np.sqrt(mc_it) >= mean_precision[i + algo_offset] - 1.96*std_precision[i + algo_offset]/np.sqrt(mc_it), \n",
    "            facecolor=colors[i], \n",
    "            alpha=0.15,\n",
    "            interpolate=True)\n",
    "    \n",
    "algo_offset=3\n",
    "for i, algo in enumerate(algorithms[algo_offset:]):\n",
    "    ax[1,1].plot(ns, mean_precision[i + algo_offset], label=algo, c=colors[i + algo_offset])\n",
    "    ax[1,1].fill_between(ns, \n",
    "            mean_precision[i + algo_offset] + 1.96*std_precision[i + algo_offset]/np.sqrt(mc_it), \n",
    "            mean_precision[i + algo_offset] - 1.96*std_precision[i + algo_offset]/np.sqrt(mc_it), \n",
    "            where=mean_precision[i + algo_offset] + 1.96*std_precision[i + algo_offset]/np.sqrt(mc_it) >= mean_precision[i + algo_offset] - 1.96*std_precision[i + algo_offset]/np.sqrt(mc_it), \n",
    "            facecolor=colors[i +algo_offset], \n",
    "            alpha=0.15,\n",
    "            interpolate=True)\n",
    "    \n",
    "ax[1,0].set_ylabel('Precision')\n",
    "ax[0,1].set_xticks([])\n",
    "ax[0,1].set_yticks([])\n",
    "ax[1,0].set_ylim(0 - 0.01)\n",
    "ax[1,0].set_ylim(0 - 0.01)\n",
    "\n",
    "algo_offset=0\n",
    "for i, algo in enumerate(algorithms[algo_offset:-3]):\n",
    "    ax[2,0].plot(ns, mean_recall[i + algo_offset], label=algo, c=colors[i])\n",
    "    ax[2,0].fill_between(ns, \n",
    "            mean_recall[i + algo_offset] + 1.96*std_recall[i + algo_offset]/np.sqrt(mc_it), \n",
    "            mean_recall[i + algo_offset] - 1.96*std_recall[i + algo_offset]/np.sqrt(mc_it), \n",
    "            where=mean_recall[i + algo_offset] + 1.96*std_recall[i + algo_offset]/np.sqrt(mc_it) >= mean_recall[i + algo_offset] - 1.96*std_recall[i + algo_offset]/np.sqrt(mc_it), \n",
    "            facecolor=colors[i], \n",
    "            alpha=0.15,\n",
    "            interpolate=True)\n",
    "    \n",
    "algo_offset=3\n",
    "for i, algo in enumerate(algorithms[algo_offset:]):\n",
    "    ax[2,1].plot(ns, mean_recall[i + algo_offset], label=algo, c=colors[i + algo_offset])\n",
    "    ax[2,1].fill_between(ns, \n",
    "            mean_recall[i + algo_offset] + 1.96*std_recall[i + algo_offset]/np.sqrt(mc_it), \n",
    "            mean_recall[i + algo_offset] - 1.96*std_recall[i + algo_offset]/np.sqrt(mc_it), \n",
    "            where=mean_recall[i + algo_offset] + 1.96*std_recall[i + algo_offset]/np.sqrt(mc_it) >= mean_recall[i + algo_offset] - 1.96*std_recall[i + algo_offset]/np.sqrt(mc_it), \n",
    "            facecolor=colors[i +algo_offset], \n",
    "            alpha=0.15,\n",
    "            interpolate=True)\n",
    "    \n",
    "ax[2,0].set_ylabel('Recall')\n",
    "ax[0,1].set_xticks([])\n",
    "ax[0,1].set_yticks([])\n",
    "ax[2,0].set_ylim(0 - 0.01)\n",
    "ax[2,0].set_ylim(0 - 0.01)\n",
    "\n",
    "algo_offset=0\n",
    "for i, algo in enumerate(algorithms[algo_offset:-3]):\n",
    "    ax[3,0].plot(ns, mean_accuracy[i + algo_offset], label=algo, c=colors[i])\n",
    "    ax[3,0].fill_between(ns, \n",
    "            mean_accuracy[i + algo_offset] + 1.96*std_accuracy[i + algo_offset]/np.sqrt(mc_it), \n",
    "            mean_accuracy[i + algo_offset] - 1.96*std_accuracy[i + algo_offset]/np.sqrt(mc_it), \n",
    "            where=mean_accuracy[i + algo_offset] + 1.96*std_accuracy[i + algo_offset]/np.sqrt(mc_it) >= mean_accuracy[i + algo_offset] - 1.96*std_accuracy[i + algo_offset]/np.sqrt(mc_it), \n",
    "            facecolor=colors[i], \n",
    "            alpha=0.15,\n",
    "            interpolate=True)\n",
    "    \n",
    "algo_offset=3\n",
    "for i, algo in enumerate(algorithms[algo_offset:]):\n",
    "    ax[3,1].plot(ns, mean_accuracy[i + algo_offset], label=algo, c=colors[i + algo_offset])\n",
    "    ax[3,1].fill_between(ns, \n",
    "            mean_accuracy[i + algo_offset] + 1.96*std_accuracy[i + algo_offset]/np.sqrt(mc_it), \n",
    "            mean_accuracy[i + algo_offset] - 1.96*std_accuracy[i + algo_offset]/np.sqrt(mc_it), \n",
    "            where=mean_accuracy[i + algo_offset] + 1.96*std_accuracy[i + algo_offset]/np.sqrt(mc_it) >= mean_accuracy[i + algo_offset] - 1.96*std_accuracy[i + algo_offset]/np.sqrt(mc_it), \n",
    "            facecolor=colors[i +algo_offset], \n",
    "            alpha=0.15,\n",
    "            interpolate=True)\n",
    "    \n",
    "ax[3,0].legend(loc='lower center', fontsize=14.5)\n",
    "ax[3,1].legend(loc='lower center', fontsize=14.5)\n",
    "    \n",
    "ax[3,0].set_ylabel('Accuracy')\n",
    "ax[0,1].set_xticks([])\n",
    "ax[0,1].set_yticks([])\n",
    "ax[3,0].set_ylim(0 - 0.01)\n",
    "ax[3,0].set_ylim(0 - 0.01)\n",
    "\n",
    "ax[1,0].tick_params(labelbottom=False, labelleft=False)\n",
    "ax[2,0].tick_params(labelbottom=False, labelleft=False)\n",
    "# ax[3,0].tick_params(labelleft=False)\n",
    "\n",
    "ax[1,1].tick_params(labelbottom=False)\n",
    "ax[2,1].tick_params(labelbottom=False)\n",
    "ax[3,1].set_yticks([0,1])\n",
    "\n",
    "# ax[1,1].set_xticks([])\n",
    "# ax[2,1].set_xticks([])\n",
    "# ax[3,1].set_xticks([])\n",
    "\n",
    "# ax[2,1].set_yticks([])\n",
    "# ax[3,1].set_yticks([])\n",
    "\n",
    "fig1.subplots_adjust(wspace=0.05, hspace=0.1)\n",
    "fig1.text(0.5, 0.065, 'Target Task Sample Size', ha='center')\n",
    "\n",
    "\n",
    "plt.savefig('PRA_parity_notparity_3d_n200.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 1/11 [00:02<00:27,  2.75s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 18%|█▊        | 2/11 [00:05<00:23,  2.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 27%|██▋       | 3/11 [00:07<00:21,  2.65s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 36%|███▋      | 4/11 [00:10<00:18,  2.71s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▌     | 5/11 [00:13<00:17,  2.89s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████▍    | 6/11 [00:17<00:15,  3.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "np.random.seed(1)\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "mc_its = 100 # number of simulation repititions\n",
    "# ns0 = (10*np.arange(3,5,step=1)).astype(int)\n",
    "ns = (100*np.arange(0.5, 6, step=0.5)).astype(int) # number of training samples \n",
    "nz_prop = 1\n",
    "\n",
    "m = 100 # number of test samples each monte carlo iteration\n",
    "\n",
    "algorithms = ['Decision Forests Source', 'Lifelong Forests Source', 'Optimal Forests Source']\n",
    "algorithms = np.concatenate((algorithms, ['Decision Forests Target', 'Lifelong Forests Target', 'Optimal Forests Target']))\n",
    "\n",
    "M = len(algorithms)\n",
    "\n",
    "mean_precision= np.zeros((M, len(ns)))\n",
    "std_precision= np.zeros((M, len(ns)))\n",
    "\n",
    "d = 3\n",
    "metric = 'precision'\n",
    "np.random.seed(1)\n",
    "for j, n in enumerate(tqdm(ns)):\n",
    "    condensed_func = lambda x : jovo_experiment(200, x, d, generate_parity, generate_box, m, metric=metric,\n",
    "                                                n_algos=M)\n",
    "    \n",
    "    precisions = np.array(Parallel(n_jobs=-2)(delayed(condensed_func)(int(x)) for x in n*np.ones(mc_its)))\n",
    "    \n",
    "    mean_precision[:, j] = np.mean(precisions, axis=0)\n",
    "    std_precision[:, j] = np.std(precisions, ddof=1, axis=0)\n",
    "\n",
    "mean_recall= np.zeros((M, len(ns)))\n",
    "std_recall= np.zeros((M, len(ns)))\n",
    "\n",
    "d = 3\n",
    "metric = 'recall'\n",
    "np.random.seed(1)\n",
    "for j, n in enumerate(tqdm(ns)):\n",
    "    condensed_func = lambda x : jovo_experiment(200, x, d, generate_parity, generate_box, m, metric=metric,\n",
    "                                                n_algos=M)\n",
    "    \n",
    "    recalls = np.array(Parallel(n_jobs=-2)(delayed(condensed_func)(int(x)) for x in n*np.ones(mc_its)))\n",
    "    \n",
    "    mean_recall[:, j] = np.mean(recalls, axis=0)\n",
    "    std_recall[:, j] = np.std(recalls, ddof=1, axis=0)\n",
    "\n",
    "mean_accuracy= np.zeros((M, len(ns)))\n",
    "std_accuracy= np.zeros((M, len(ns)))\n",
    "\n",
    "d = 3\n",
    "metric = 'accuracy'\n",
    "np.random.seed(1)\n",
    "for j, n in enumerate(tqdm(ns)):\n",
    "    condensed_func = lambda x : jovo_experiment(200, x, d, generate_parity, generate_box, m, metric=metric,\n",
    "                                                n_algos=M)\n",
    "    \n",
    "    accuracys = np.array(Parallel(n_jobs=-2)(delayed(condensed_func)(int(x)) for x in n*np.ones(mc_its)))\n",
    "    \n",
    "    mean_accuracy[:, j] = np.mean(accuracys, axis=0)\n",
    "    std_accuracy[:, j] = np.std(accuracys, ddof=1, axis=0)\n",
    "\n",
    "np.random.seed(2)\n",
    "X, labelsX = generate_parity(500, 2)\n",
    "temp_labelsX = []\n",
    "\n",
    "# Target task\n",
    "Z, labelsZ = generate_box(500, 2, invert_labels=True)\n",
    "temp_labelsZ = []\n",
    "\n",
    "for i in range(len(labelsX)):\n",
    "    if labelsX[i] == 0:\n",
    "        temp_labelsX.append('k')\n",
    "    else:\n",
    "        temp_labelsX.append('pink')\n",
    "        \n",
    "    if labelsZ[i] == 0:\n",
    "        temp_labelsZ.append('pink')\n",
    "    else:\n",
    "        temp_labelsZ.append('k')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "mc_it = mc_its\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "colors = sns.color_palette(\"Set1\", n_colors = M)\n",
    "fig1, ax = plt.subplots(4,2, figsize = (13, 13), sharex = 'row', sharey='row')\n",
    "\n",
    "ax[0,0].scatter(X[:, 0], X[:, 1], c = temp_labelsX)\n",
    "ax[0,1].scatter(Z[:, 0], Z[:, 1], c = temp_labelsZ)\n",
    "\n",
    "\n",
    "ax[0,0].set_title('Source Task: Parity')\n",
    "ax[0,1].set_title('Target Task: In-Out')\n",
    "    \n",
    "algo_offset=0\n",
    "for i, algo in enumerate(algorithms[algo_offset:-3]):\n",
    "    ax[1,0].plot(ns, mean_precision[i + algo_offset], label=algo, c=colors[i])\n",
    "    ax[1,0].fill_between(ns, \n",
    "            mean_precision[i + algo_offset] + 1.96*std_precision[i + algo_offset]/np.sqrt(mc_it), \n",
    "            mean_precision[i + algo_offset] - 1.96*std_precision[i + algo_offset]/np.sqrt(mc_it), \n",
    "            where=mean_precision[i + algo_offset] + 1.96*std_precision[i + algo_offset]/np.sqrt(mc_it) >= mean_precision[i + algo_offset] - 1.96*std_precision[i + algo_offset]/np.sqrt(mc_it), \n",
    "            facecolor=colors[i], \n",
    "            alpha=0.15,\n",
    "            interpolate=True)\n",
    "    \n",
    "algo_offset=3\n",
    "for i, algo in enumerate(algorithms[algo_offset:]):\n",
    "    ax[1,1].plot(ns, mean_precision[i + algo_offset], label=algo, c=colors[i + algo_offset])\n",
    "    ax[1,1].fill_between(ns, \n",
    "            mean_precision[i + algo_offset] + 1.96*std_precision[i + algo_offset]/np.sqrt(mc_it), \n",
    "            mean_precision[i + algo_offset] - 1.96*std_precision[i + algo_offset]/np.sqrt(mc_it), \n",
    "            where=mean_precision[i + algo_offset] + 1.96*std_precision[i + algo_offset]/np.sqrt(mc_it) >= mean_precision[i + algo_offset] - 1.96*std_precision[i + algo_offset]/np.sqrt(mc_it), \n",
    "            facecolor=colors[i +algo_offset], \n",
    "            alpha=0.15,\n",
    "            interpolate=True)\n",
    "    \n",
    "ax[1,0].set_ylabel('Precision')\n",
    "ax[0,1].set_xticks([])\n",
    "ax[0,1].set_yticks([])\n",
    "ax[1,0].set_ylim(0 - 0.01)\n",
    "ax[1,0].set_ylim(0 - 0.01)\n",
    "\n",
    "algo_offset=0\n",
    "for i, algo in enumerate(algorithms[algo_offset:-3]):\n",
    "    ax[2,0].plot(ns, mean_recall[i + algo_offset], label=algo, c=colors[i])\n",
    "    ax[2,0].fill_between(ns, \n",
    "            mean_recall[i + algo_offset] + 1.96*std_recall[i + algo_offset]/np.sqrt(mc_it), \n",
    "            mean_recall[i + algo_offset] - 1.96*std_recall[i + algo_offset]/np.sqrt(mc_it), \n",
    "            where=mean_recall[i + algo_offset] + 1.96*std_recall[i + algo_offset]/np.sqrt(mc_it) >= mean_recall[i + algo_offset] - 1.96*std_recall[i + algo_offset]/np.sqrt(mc_it), \n",
    "            facecolor=colors[i], \n",
    "            alpha=0.15,\n",
    "            interpolate=True)\n",
    "    \n",
    "algo_offset=3\n",
    "for i, algo in enumerate(algorithms[algo_offset:]):\n",
    "    ax[2,1].plot(ns, mean_recall[i + algo_offset], label=algo, c=colors[i + algo_offset])\n",
    "    ax[2,1].fill_between(ns, \n",
    "            mean_recall[i + algo_offset] + 1.96*std_recall[i + algo_offset]/np.sqrt(mc_it), \n",
    "            mean_recall[i + algo_offset] - 1.96*std_recall[i + algo_offset]/np.sqrt(mc_it), \n",
    "            where=mean_recall[i + algo_offset] + 1.96*std_recall[i + algo_offset]/np.sqrt(mc_it) >= mean_recall[i + algo_offset] - 1.96*std_recall[i + algo_offset]/np.sqrt(mc_it), \n",
    "            facecolor=colors[i +algo_offset], \n",
    "            alpha=0.15,\n",
    "            interpolate=True)\n",
    "    \n",
    "ax[2,0].set_ylabel('Recall')\n",
    "ax[0,1].set_xticks([])\n",
    "ax[0,1].set_yticks([])\n",
    "ax[2,0].set_ylim(0 - 0.01)\n",
    "ax[2,0].set_ylim(0 - 0.01)\n",
    "\n",
    "algo_offset=0\n",
    "for i, algo in enumerate(algorithms[algo_offset:-3]):\n",
    "    ax[3,0].plot(ns, mean_accuracy[i + algo_offset], label=algo, c=colors[i])\n",
    "    ax[3,0].fill_between(ns, \n",
    "            mean_accuracy[i + algo_offset] + 1.96*std_accuracy[i + algo_offset]/np.sqrt(mc_it), \n",
    "            mean_accuracy[i + algo_offset] - 1.96*std_accuracy[i + algo_offset]/np.sqrt(mc_it), \n",
    "            where=mean_accuracy[i + algo_offset] + 1.96*std_accuracy[i + algo_offset]/np.sqrt(mc_it) >= mean_accuracy[i + algo_offset] - 1.96*std_accuracy[i + algo_offset]/np.sqrt(mc_it), \n",
    "            facecolor=colors[i], \n",
    "            alpha=0.15,\n",
    "            interpolate=True)\n",
    "    \n",
    "algo_offset=3\n",
    "for i, algo in enumerate(algorithms[algo_offset:]):\n",
    "    ax[3,1].plot(ns, mean_accuracy[i + algo_offset], label=algo, c=colors[i + algo_offset])\n",
    "    ax[3,1].fill_between(ns, \n",
    "            mean_accuracy[i + algo_offset] + 1.96*std_accuracy[i + algo_offset]/np.sqrt(mc_it), \n",
    "            mean_accuracy[i + algo_offset] - 1.96*std_accuracy[i + algo_offset]/np.sqrt(mc_it), \n",
    "            where=mean_accuracy[i + algo_offset] + 1.96*std_accuracy[i + algo_offset]/np.sqrt(mc_it) >= mean_accuracy[i + algo_offset] - 1.96*std_accuracy[i + algo_offset]/np.sqrt(mc_it), \n",
    "            facecolor=colors[i +algo_offset], \n",
    "            alpha=0.15,\n",
    "            interpolate=True)\n",
    "    \n",
    "ax[3,0].legend(loc='lower center', fontsize=14.5)\n",
    "ax[3,1].legend(loc='lower center', fontsize=14.5)\n",
    "    \n",
    "ax[3,0].set_ylabel('Accuracy')\n",
    "ax[0,1].set_xticks([])\n",
    "ax[0,1].set_yticks([])\n",
    "ax[3,0].set_ylim(0 - 0.01)\n",
    "ax[3,0].set_ylim(0 - 0.01)\n",
    "\n",
    "ax[1,0].tick_params(labelbottom=False, labelleft=False)\n",
    "ax[2,0].tick_params(labelbottom=False, labelleft=False)\n",
    "# ax[3,0].tick_params(labelleft=False)\n",
    "\n",
    "ax[1,1].tick_params(labelbottom=False)\n",
    "ax[2,1].tick_params(labelbottom=False)\n",
    "ax[3,1].set_yticks([0,1])\n",
    "\n",
    "# ax[1,1].set_xticks([])\n",
    "# ax[2,1].set_xticks([])\n",
    "# ax[3,1].set_xticks([])\n",
    "\n",
    "# ax[2,1].set_yticks([])\n",
    "# ax[3,1].set_yticks([])\n",
    "\n",
    "fig1.subplots_adjust(wspace=0.05, hspace=0.1)\n",
    "fig1.text(0.5, 0.065, 'Target Task Sample Size', ha='center')\n",
    "\n",
    "\n",
    "plt.savefig('PRA_parity_inout_3d_n200.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
