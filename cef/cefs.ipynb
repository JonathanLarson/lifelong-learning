{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble.forest import _generate_unsampled_indices\n",
    "from sklearn.ensemble.forest import _generate_sample_indices\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from itertools import permutations\n",
    "\n",
    "import numpy as np\n",
    "np.warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook investigates transfer learning (and, eventually, lifelong learning) using three methods: Ignoring possible useful information in disparate data sets, using the structure in learned trees to \"transfer\" information, and comparing two classifiers to assess how similar the underlying distributions are.\n",
    "\n",
    "In the two latter methods, the ideas underlying Guo's Conditional Entropy Forests are used to estimate posteriors.\n",
    "\n",
    "To see a full description of the task, see https://github.com/neurodata/lifelong-learning/blob/master/willy_nilly.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file contains code for conditional entropy forests\n",
    "\"\"\"\n",
    "\n",
    "def finite_sample_correction(class_probs, row_sums):\n",
    "    \n",
    "    where_0 = np.argwhere(class_probs == 0)\n",
    "    for elem in where_0:\n",
    "        class_probs[elem[0], elem[1]] = 1 / (2 * row_sums[elem[0], None])\n",
    "    where_1 = np.argwhere(class_probs == 1)\n",
    "    for elem in where_1:\n",
    "        class_probs[elem[0], elem[1]] = 1 - 1 / (2 * row_sums[elem[0], None])\n",
    "    \n",
    "    return class_probs\n",
    "\n",
    "def build_model(X, y, n_estimators=200, max_samples=.32,\n",
    "                                            bootstrap=True,\n",
    "                                            depth=30,\n",
    "                                            min_samples_leaf=1):\n",
    "    if X.ndim == 1:\n",
    "        X=X.reshape(-1, 1)\n",
    "        \n",
    "    model=BaggingClassifier(DecisionTreeClassifier(max_depth=depth, min_samples_leaf=min_samples_leaf,\n",
    "                                                     max_features = np.ceil(int(np.sqrt(X.shape[1])))),\n",
    "                              n_estimators=n_estimators,\n",
    "                              max_samples=max_samples,\n",
    "                              bootstrap=bootstrap)\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "def estimate_posteriors(model, train, y, test, in_task=True, acorn=None):\n",
    "    if acorn is None:\n",
    "        acorn = np.random.randint(10**6)\n",
    "    np.random.seed(acorn)\n",
    "    \n",
    "    n, d = train.shape\n",
    "    m, d_ = test.shape\n",
    "    \n",
    "    if d != d_:\n",
    "        raise ValueError(\"train and test data in different dimensions\")\n",
    "    \n",
    "    class_counts = np.zeros((m, model.n_classes_))\n",
    "    for tree in model:\n",
    "        # get out of bag indicies\n",
    "        if in_task:\n",
    "            prob_indices = _generate_unsampled_indices(tree.random_state, n)\n",
    "            # in_bag_idx = _generate_sample_indices(tree.random_state, n) # this is not behaving as i expected\n",
    "        else:\n",
    "            prob_indices = range(n)\n",
    "            in_bag_idx = range(n)\n",
    "        \n",
    "        all_leaf_nodes = tree.apply(train)\n",
    "        unique_leaf_nodes = np.unique(all_leaf_nodes)\n",
    "            \n",
    "        # get all node counts\n",
    "        node_counts = tree.tree_.n_node_samples\n",
    "        # get probs for eval samples\n",
    "        posterior_class_counts = np.zeros((len(unique_leaf_nodes), model.n_classes_))\n",
    "\n",
    "        for prob_index in prob_indices:\n",
    "            temp_node = tree.apply(train[prob_index].reshape(1, -1)).item()\n",
    "            posterior_class_counts[np.where(unique_leaf_nodes == temp_node)[0][0], y[prob_index]] += 1\n",
    "            \n",
    "        # total number of points in a node\n",
    "        row_sums = posterior_class_counts.sum(axis=1)\n",
    "        \n",
    "        # no divide by zero\n",
    "        row_sums[row_sums == 0] = 1\n",
    "\n",
    "        # posteriors\n",
    "        class_probs = (posterior_class_counts / row_sums[:, None])\n",
    "        # posteriors with finite sampling correction\n",
    "        \n",
    "        class_probs = finite_sample_correction(class_probs, row_sums)\n",
    "    \n",
    "        # posteriors as a list\n",
    "        class_probs.tolist()\n",
    "        partition_counts = np.asarray([node_counts[np.where(unique_leaf_nodes == x)[0][0]] for x in tree.apply(test)])\n",
    "        # get probability for out of bag samples\n",
    "        eval_class_probs = [class_probs[np.where(unique_leaf_nodes == x)[0][0]] for x in tree.apply(test)]\n",
    "        eval_class_probs = np.array(eval_class_probs)\n",
    "        # find total elements for out of bag samples\n",
    "        elems = np.multiply(eval_class_probs, partition_counts[:, np.newaxis])\n",
    "        # store counts for each x (repeat fhis for each tree)\n",
    "        class_counts += elems\n",
    "    # calculate p(y|X = x) for all x's\n",
    "    probs = class_counts / class_counts.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    return probs\n",
    "\n",
    "def predict(posteriors):\n",
    "    return np.argmax(s, axis = 1)\n",
    "\n",
    "def permutation(predict1, predict2):\n",
    "    \"\"\"\n",
    "    how to use:\n",
    "    \n",
    "    this function returns the permutation i.e. \\pi: [K] -> [K] that maximizes\n",
    "    the number of matched predictions\n",
    "    \n",
    "    to use the permutation for posteriors for point i (posterior_i), say, simply use\n",
    "    posterior_i[permutation]\n",
    "    \n",
    "    \"\"\"\n",
    "    unique_1 = np.unique(predict1)\n",
    "    unique_2 = np.unique(predict2)\n",
    "    \n",
    "    if set(unique_1) != set(unique_2):\n",
    "        raise ValueError(\"predictions must be on the same set of labels\")\n",
    "        \n",
    "    K = len(unique_1)\n",
    "    \n",
    "    max_sum = 0\n",
    "    max_perm = unique_2\n",
    "    for i, perm in enumerate(permutations(unique_2)):\n",
    "        perm = np.array(list(perm))\n",
    "        temp_predict2 = -1*np.ones(len(predict2))\n",
    "        \n",
    "        for k in range(K):\n",
    "            temp_predict2[np.where(predict2 == unique_2[k])[0]] = perm[k]\n",
    "           \n",
    "        temp_sum = np.sum(predict1 == temp_predict2)\n",
    "        if temp_sum > max_sum:\n",
    "            max_sum = temp_sum\n",
    "            max_perm = perm\n",
    "            \n",
    "    return max_perm\n",
    "            \n",
    "def estimate_alpha(predict1, predict2, permutation=None):\n",
    "    if permutation is None:\n",
    "        return np.sum(predict1 == predict2) / len(predict1)\n",
    "    else:\n",
    "        unique = np.unique(temp_predict2)\n",
    "        temp_predict2 = -1*np.ones(len(predict2))\n",
    "        \n",
    "        for i, k in enumerate(unique):\n",
    "            temp_predict2[predict2 == k] = permutation[i]\n",
    "            \n",
    "        return np.sum(predict1 == temp_predict2) / len(predict1)\n",
    "    \n",
    "def generate_sample(n, pi, conditional_0, params0, conditional_1, params1, d = 2, acorn=None):\n",
    "    if acorn is None:\n",
    "        acorn = np.random.seed(10**6)\n",
    "    np.random.seed(acorn)\n",
    "    \n",
    "    n0 = int(np.random.binomial(n, pi))\n",
    "    n1 = n - n0\n",
    "    ns = [n0, n1]\n",
    "    \n",
    "    X0 = conditional_0(*params0, size=(n0, d))\n",
    "    \n",
    "    X1 = conditional_1(*params1, size=(n1, d))\n",
    "    \n",
    "    labels = np.concatenate([i*np.ones(ns[i]) for i in range(len(ns))]).astype(int)\n",
    "    \n",
    "    return [np.concatenate((X0, X1), axis = 0), labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-336-0e7bad75838f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mposteriors_structX_estX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimate_posteriors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelsX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_task\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mposteriors_structZ_estX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimate_posteriors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_Z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelsX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_task\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mpred_structX_estX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposteriors_structX_estX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-334-3839e417bdb9>\u001b[0m in \u001b[0;36mestimate_posteriors\u001b[0;34m(model, train, y, test, in_task, acorn)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# posteriors as a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mclass_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mpartition_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_leaf_nodes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;31m# get probability for out of bag samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0meval_class_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclass_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_leaf_nodes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-334-3839e417bdb9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# posteriors as a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mclass_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mpartition_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_leaf_nodes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;31m# get probability for out of bag samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0meval_class_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclass_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_leaf_nodes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "mc_its = 100 # number of simulation repititions\n",
    "ns0 = 10*np.arange(1,6,step=1).astype(int)\n",
    "ns1 = 100*np.arange(0.5, 5.5, step=0.5).astype(int) # number of training samples \n",
    "ns = np.concatenate((ns0, ns1))\n",
    "n = 100\n",
    "nz_prop = 1\n",
    "\n",
    "m = 1000 # number of test samples each monte carlo iteration\n",
    "\n",
    "pi = 0.5\n",
    "cond_X_0, cond_X_1 = 2*[np.random.uniform]\n",
    "params_X_0, params_X_1 = [0, 0.5], [0.5, 1]\n",
    "\n",
    "cond_Z_0, cond_Z_1 = 2*[np.random.uniform]\n",
    "params_Z_0, params_Z_1 = [0.5, 1], [0, 0.5]\n",
    "\n",
    "algorithms = ['silly willy nilly t1', 'silly willy nilly t2', 'task1', 'task2', 'task1 cep global', 'task2 cep global', 'task1 jtv', 'task2 jtv']\n",
    "algorithms = np.concatenate((algorithms, ['task1 cep local', 'task2 cep local', 'task1 jtv', 'task2 jtv']))\n",
    "\n",
    "errors = [[[] for _ in ns] for __ in algorithms]\n",
    "mean_error = np.zeros((len(algorithms), len(ns)))\n",
    "std_error = np.zeros((len(algorithms), len(ns)))\n",
    "\n",
    "for j, n in enumerate(tqdm(ns)):\n",
    "    errors = [[[] for _ in ns] for __ in algorithms]\n",
    "    \n",
    "    k = int(np.floor(np.log(n)))\n",
    "    T = int(np.floor(np.sqrt(n)))\n",
    "    \n",
    "    for i in range(mc_its):\n",
    "        temp_predictions = [[] for __ in algorithms]\n",
    "        \n",
    "        X, labelsX = generate_sample(n, pi, cond_X_0, params_X_0, cond_X_1, params_X_1)\n",
    "        testX, test_labelsX = generate_sample(m, pi, cond_X_0, params_X_0, cond_X_1, params_X_1)\n",
    "        \n",
    "        Z, labelsZ = generate_sample(n, pi, cond_Z_0, params_Z_0, cond_Z_1, params_Z_1)\n",
    "        testZ, test_labelsZ = generate_sample(m, pi, cond_Z_0, params_Z_0, cond_Z_1, params_Z_1)\n",
    "        \n",
    "        joint, joint_labels = np.concatenate((X, Z)), np.concatenate((labelsX, labelsZ))\n",
    "        joint_test = np.concatenate((testX, testZ), axis = 0)\n",
    "        \n",
    "        model_joint = build_model(joint, joint_labels)\n",
    "        model_X = build_model(X, labelsX)\n",
    "        model_Z = build_model(Z, labelsZ)\n",
    "        \n",
    "        posteriors_structjoint_estjoint=estimate_posteriors(model_joint, joint, joint_labels, joint_test, in_task=True)\n",
    "        predictions_joint = predict(posteriors_structjoint_estjoint)\n",
    "        \n",
    "        # calculate errors for jointly learned forests\n",
    "        errors[0][i].append(np.sum(test_labelsX == predictions_joint[range(n)])/n)\n",
    "        errors[1][i].append(np.sum(test_labelsZ == predictions_joint[range(n, 2*n)])/n)\n",
    "        \n",
    "        posteriors_structX_estX=estimate_posteriors(model_X, X, labelsX, testX, in_task=True)\n",
    "        posteriors_structZ_estX=estimate_posteriors(model_Z, X, labelsX, testX, in_task=False)\n",
    "        \n",
    "        pred_structX_estX=predict(posteriors_structX_estX)\n",
    "        pred_structZ_estX=predict(posteriors_structZ_estX)\n",
    "        \n",
    "        posteriors_structX_estZ=estimate_posteriors(model_X, Z, labelsZ, testZ, in_task=False)\n",
    "        posteriors_structZ_estZ=estimate_posteriors(model_Z, Z, labelsZ, testZ, in_task=True)\n",
    "        \n",
    "        pred_structX_estZ=predict(posteriors_structX_estZ)\n",
    "        pred_structZ_estZ=predict(posteriors_structZ_estZ)\n",
    "        \n",
    "        # calculate errors without attempting to transfer knowledge\n",
    "        pred_X = predict(posteriors_structX_estX)\n",
    "        pred_Z = predict(posteriors_structZ_estZ)\n",
    "        \n",
    "        errors[2][i].append(np.sum(test_labelsX == pred_X)/n)\n",
    "        errors[3][i].append(np.sum(test_labelsZ == pred_Z)/n)\n",
    "        \n",
    "        # jtv ?\n",
    "        \n",
    "        # cep global\n",
    "        optimal_permutation_X = permutation(pred_structX_estX, pred_structZ_estX)\n",
    "        optimal_permutation_Z = permutation(pred_structZ_estZ, pred_structX_estZ)\n",
    "        \n",
    "        new_posteriors_structX_estZ = np.zeros(posteriors_structX_estZ.shape)\n",
    "        for i in range(n):\n",
    "            new_posteriors_structX_estZ[i] = posteriors_structX_estZ[i][optimal_permutation_X]\n",
    "            new_posteriors_structZ_estX[i] = posteriors_structZ_estX[i][optimal_permutation_Z]\n",
    "            \n",
    "        pred_structX_estZ = predict(posteriors_structX_estZ)\n",
    "        pred_structZ_estX = predict(posteriors_structZ_estX)\n",
    "        \n",
    "        alpha_X = estimate_alpha(pred_structX_estX, pred_structZ_estX)\n",
    "        alpha_Z = estimate_alpha(pred_structZ_estZ, pred_structX_estZ)\n",
    "            \n",
    "        pred_X_cep_global = predict(posteriors_structX_estX + alpha_X * new_posteriors_structZ_estX)\n",
    "        pred_Z_cep_global = predict(posteriors_structZ_estZ + alpha_Z * new_posteriors_structX_estZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(X, labelsX)\n",
    "s = estimate_posteriors(model, X, labelsX, testZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "s = estimate_posteriors(model, Z, labelsZ, testZ, in_task=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hh",
   "language": "python",
   "name": "hh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
